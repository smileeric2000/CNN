{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b948438-6c50-4498-b76a-e4b1c3097634",
   "metadata": {},
   "source": [
    "## PyTorch Basics\n",
    "#### ML workflow implemented in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d9f40d-e6be-4937-bf6f-3c725c75a8f1",
   "metadata": {},
   "source": [
    "Most machine learning workflows involve working with data, creating models, optimizing model parameters, and saving the trained models.\n",
    "We’ll use the FashionMNIST dataset to train a neural network that predicts if an input image belongs to one of the following classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, or Ankle boot.\n",
    "####  setup PyTorch and TorchVision "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5920451-5902-459e-ad45-0e210222bdce",
   "metadata": {},
   "source": [
    "## Working with data\r\n",
    "PyTorch has two primitives to work with data:$ torch.utils.data.DataLoade$r and$ torch.utils.data.Datase$t.$Datase$t stores the samples and their corresponding labels, and$DDataLoade$r wraps an iterable around the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18cd3f17-0aa5-4db0-8d28-e30b66a0ded2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvisionNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading torchvision-0.21.0-cp311-cp311-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.6.0->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.6.0->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.6.0->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.6.0->torchvision) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch==2.6.0->torchvision) (2.1.3)\n",
      "Downloading torchvision-0.21.0-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.6 MB 131.3 kB/s eta 0:00:12\n",
      "    --------------------------------------- 0.0/1.6 MB 131.3 kB/s eta 0:00:12\n",
      "    --------------------------------------- 0.0/1.6 MB 131.3 kB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.0/1.6 MB 115.9 kB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.0/1.6 MB 115.9 kB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.0/1.6 MB 115.9 kB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.1/1.6 MB 157.5 kB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.1/1.6 MB 157.5 kB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.1/1.6 MB 163.8 kB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.1/1.6 MB 173.6 kB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.1/1.6 MB 173.6 kB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.1/1.6 MB 173.6 kB/s eta 0:00:09\n",
      "   --- ------------------------------------ 0.1/1.6 MB 171.7 kB/s eta 0:00:09\n",
      "   --- ------------------------------------ 0.1/1.6 MB 185.4 kB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 0.2/1.6 MB 214.2 kB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.2/1.6 MB 214.2 kB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.2/1.6 MB 222.6 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.2/1.6 MB 222.4 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.2/1.6 MB 229.5 kB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 0.2/1.6 MB 229.5 kB/s eta 0:00:06\n",
      "   ------ --------------------------------- 0.2/1.6 MB 225.3 kB/s eta 0:00:06\n",
      "   ------ --------------------------------- 0.2/1.6 MB 225.3 kB/s eta 0:00:06\n",
      "   ------ --------------------------------- 0.3/1.6 MB 228.0 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 0.3/1.6 MB 230.3 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 0.3/1.6 MB 230.3 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 0.3/1.6 MB 237.6 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 0.3/1.6 MB 237.6 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 0.3/1.6 MB 228.7 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 0.3/1.6 MB 228.7 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 0.3/1.6 MB 228.7 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 0.3/1.6 MB 228.0 kB/s eta 0:00:06\n",
      "   --------- ------------------------------ 0.4/1.6 MB 232.1 kB/s eta 0:00:06\n",
      "   --------- ------------------------------ 0.4/1.6 MB 231.7 kB/s eta 0:00:06\n",
      "   --------- ------------------------------ 0.4/1.6 MB 237.8 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.4/1.6 MB 237.8 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.4/1.6 MB 247.3 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.4/1.6 MB 252.5 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.5/1.6 MB 247.2 kB/s eta 0:00:05\n",
      "   ------------ --------------------------- 0.5/1.6 MB 254.3 kB/s eta 0:00:05\n",
      "   ------------ --------------------------- 0.5/1.6 MB 255.6 kB/s eta 0:00:05\n",
      "   ------------ --------------------------- 0.5/1.6 MB 262.1 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 0.5/1.6 MB 269.6 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 0.6/1.6 MB 271.4 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 0.6/1.6 MB 282.3 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 0.6/1.6 MB 288.0 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 0.6/1.6 MB 288.0 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 0.6/1.6 MB 284.3 kB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 0.6/1.6 MB 285.6 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 0.7/1.6 MB 291.3 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 0.7/1.6 MB 291.3 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 0.7/1.6 MB 292.3 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 0.7/1.6 MB 288.9 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 0.7/1.6 MB 291.7 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 0.7/1.6 MB 291.7 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 0.7/1.6 MB 286.7 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 0.7/1.6 MB 286.7 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 0.7/1.6 MB 286.7 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 0.7/1.6 MB 279.3 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 0.7/1.6 MB 279.3 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 0.8/1.6 MB 288.4 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 0.8/1.6 MB 301.9 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 0.9/1.6 MB 303.9 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 0.9/1.6 MB 307.8 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 0.9/1.6 MB 307.8 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 0.9/1.6 MB 307.8 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 0.9/1.6 MB 307.8 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 0.9/1.6 MB 292.1 kB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 0.9/1.6 MB 301.0 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 1.0/1.6 MB 308.0 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.0/1.6 MB 321.2 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.0/1.6 MB 326.1 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.0/1.6 MB 326.1 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.1/1.6 MB 318.3 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.1/1.6 MB 318.3 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.1/1.6 MB 324.6 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.1/1.6 MB 329.2 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.1/1.6 MB 329.2 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.2/1.6 MB 333.5 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.2/1.6 MB 333.5 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.2/1.6 MB 333.5 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.2/1.6 MB 333.5 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.2/1.6 MB 322.1 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.2/1.6 MB 324.9 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.2/1.6 MB 329.1 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.2/1.6 MB 329.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.3/1.6 MB 326.4 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.3/1.6 MB 333.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.3/1.6 MB 333.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.3/1.6 MB 333.0 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.6 MB 331.6 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.4/1.6 MB 332.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.4/1.6 MB 332.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.4/1.6 MB 332.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.4/1.6 MB 326.4 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.4/1.6 MB 325.3 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.4/1.6 MB 327.7 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.4/1.6 MB 330.1 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.5/1.6 MB 332.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.5/1.6 MB 330.0 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.5/1.6 MB 330.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.5/1.6 MB 330.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.5/1.6 MB 332.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.5/1.6 MB 331.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.6/1.6 MB 335.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.6/1.6 MB 335.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.6/1.6 MB 335.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.6/1.6 MB 335.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 322.2 kB/s eta 0:00:00\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.21.0\n"
     ]
    }
   ],
   "source": [
    "#pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff9e684-bef2-4891-bb9b-303b8cfc234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a32df6a-1937-44a5-bc6f-0bb03dcef231",
   "metadata": {},
   "source": [
    "PyTorch offers domain-specific libraries such as TorchText, TorchVision, and TorchAudio, all of which include datasets. For this tutorial, we will be using a TorchVision dataset.\n",
    "   \n",
    "    The torchvision.datasets module contains Dataset objects for many real-world vision data like CIFAR, COCO (full list here). In this tutorial, we use the FashionMNIST dataset. Every TorchVision Dataset includes two arguments: transform and target_transform to modify the samples and labels respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ebe02f8-63b2-446c-9ef9-7ef0683dcdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 26.4M/26.4M [00:51<00:00, 513kB/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 29.5k/29.5k [00:00<00:00, 126kB/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 4.42M/4.42M [00:24<00:00, 183kB/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 5.15k/5.15k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a827745b-fc70-4c91-a5f4-dd856d129e19",
   "metadata": {},
   "source": [
    "We pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "557438dd-9ec9-4ab1-8942-66f91840b519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape of x [N, C, H, W]:\n",
      " torch.Size([64, 1, 28, 28])\n",
      " Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for x, y in test_dataloader:\n",
    "    print(f\" Shape of x [N, C, H, W]:\\n {x.shape}\")\n",
    "    print(f\" Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998225b9-fc90-4b4e-835a-3e71de43185f",
   "metadata": {},
   "source": [
    "## Creating Models\r\n",
    "To define a neural network in PyTorch, we create a class that inherits from nn.Module. We define the layers of the network in the$ __inti__$_ function and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the accelerator such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b69b469e-9727-46a9-bbfe-f23d8759b4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#check device type\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbdef655-832b-49a5-854c-fb6fc3a9de7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3e4f2d-ea71-4cab-8ebd-d660340a6a28",
   "metadata": {},
   "source": [
    "## Optimizing the Model Parameters\r\n",
    "To train a model, we need a loss function and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74a36ac3-0da5-4048-bb6c-458f30f8d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a3453-4820-4f91-b424-f745c8b52ae2",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model’s parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2ece248-f95f-4da9-809c-410f8d274225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(x)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5394f32f-2742-4cd4-9138-f20a9ca7d0d5",
   "metadata": {},
   "source": [
    "We also check the model’s performance against the test dataset to ensure it is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56d6d766-3ed6-4b49-be08-4296ff8c8cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067e5031-4160-47df-af26-558ac23742df",
   "metadata": {},
   "source": [
    "The training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model’s accuracy and loss at each epoch; we’d like to see the accuracy increase and the loss decrease with every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b06db37c-ffeb-40e0-90c3-2fbb2daff0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.300275  [   64/60000]\n",
      "loss: 2.285605  [ 6464/60000]\n",
      "loss: 2.266729  [12864/60000]\n",
      "loss: 2.264498  [19264/60000]\n",
      "loss: 2.242782  [25664/60000]\n",
      "loss: 2.209967  [32064/60000]\n",
      "loss: 2.222518  [38464/60000]\n",
      "loss: 2.183590  [44864/60000]\n",
      "loss: 2.180926  [51264/60000]\n",
      "loss: 2.157121  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.1%, Avg loss: 2.141966 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.150041  [   64/60000]\n",
      "loss: 2.137763  [ 6464/60000]\n",
      "loss: 2.072348  [12864/60000]\n",
      "loss: 2.096393  [19264/60000]\n",
      "loss: 2.040276  [25664/60000]\n",
      "loss: 1.977141  [32064/60000]\n",
      "loss: 2.014658  [38464/60000]\n",
      "loss: 1.922233  [44864/60000]\n",
      "loss: 1.923998  [51264/60000]\n",
      "loss: 1.875288  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 1.854428 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.881334  [   64/60000]\n",
      "loss: 1.854585  [ 6464/60000]\n",
      "loss: 1.727232  [12864/60000]\n",
      "loss: 1.783282  [19264/60000]\n",
      "loss: 1.673020  [25664/60000]\n",
      "loss: 1.624694  [32064/60000]\n",
      "loss: 1.659296  [38464/60000]\n",
      "loss: 1.549497  [44864/60000]\n",
      "loss: 1.572932  [51264/60000]\n",
      "loss: 1.495735  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 1.498728 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.554693  [   64/60000]\n",
      "loss: 1.531733  [ 6464/60000]\n",
      "loss: 1.380545  [12864/60000]\n",
      "loss: 1.464821  [19264/60000]\n",
      "loss: 1.344182  [25664/60000]\n",
      "loss: 1.341715  [32064/60000]\n",
      "loss: 1.362926  [38464/60000]\n",
      "loss: 1.281428  [44864/60000]\n",
      "loss: 1.313981  [51264/60000]\n",
      "loss: 1.235246  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 1.252597 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.319903  [   64/60000]\n",
      "loss: 1.310184  [ 6464/60000]\n",
      "loss: 1.147791  [12864/60000]\n",
      "loss: 1.256851  [19264/60000]\n",
      "loss: 1.131904  [25664/60000]\n",
      "loss: 1.157686  [32064/60000]\n",
      "loss: 1.180915  [38464/60000]\n",
      "loss: 1.112068  [44864/60000]\n",
      "loss: 1.149538  [51264/60000]\n",
      "loss: 1.082039  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.096001 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aac5986-d982-489b-8f46-68e06d56d732",
   "metadata": {},
   "source": [
    "## Saving Models\r\n",
    "A common way to save a model is to serialize the internal state dictionary (containing the model parameters).th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59cbfc83-00c8-4474-902b-a6fbaaaf150b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to SmilesPyTorchModel.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.save(model.state_dict(), \"SmilesPyTorchModel.pth\")\n",
    "print(\"Saved PyTorch Model State to SmilesPyTorchModel.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8632beee-2954-4754-ae5f-0e8744c1d0b6",
   "metadata": {},
   "source": [
    "## Loading Models\r\n",
    "The process for loading a model includes re-creating the model structure and loading the state dictionary into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91e97d50-2f48-4874-82be-8319550fbb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"SmilesPyTorchModel.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3ddaf6d-5a05-4be3-b944-0fca4d347186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5625b8ed-f6b8-4f4c-bc38-c9fa46c9ca96",
   "metadata": {},
   "source": [
    "## Make Predictions with custom made model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "234d48b2-3c80-49ef-aba6-a4fb0ad4bd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x1, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad(): \n",
    "    x1 = x1.to(device)\n",
    "    pred = model(x1)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca5e7c85-e457-4987-b1dc-e793d67ddd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0824, 0.4824, 0.4235, 0.3882, 0.3882, 0.3294, 0.3255,\n",
       "           0.3373, 0.3608, 0.2745, 0.0235, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.4157, 0.9725, 0.9020, 0.8039, 0.9373, 0.8314, 0.6824,\n",
       "           0.8431, 0.8118, 0.5451, 0.3647, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.4980, 0.8471, 0.8353, 0.8039, 0.8392, 0.8392, 0.7569,\n",
       "           0.8980, 0.7882, 0.6471, 0.3882, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.5725, 0.7647, 0.8980, 0.8314, 0.8941, 0.8431, 0.8196,\n",
       "           0.9020, 0.8392, 0.6431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.4510, 0.7569, 0.8902, 0.8196, 0.8510, 0.8196, 0.8314,\n",
       "           0.8078, 0.8784, 0.6471, 0.0549, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.4471, 0.5804, 0.7137, 0.7176, 0.8549, 0.8863, 0.8941,\n",
       "           0.7451, 0.8078, 0.5137, 0.0392, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.4392, 0.4667, 0.4157, 0.4118, 0.7412, 0.8157, 0.9843,\n",
       "           0.6471, 0.5451, 0.4078, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2784, 0.5412, 0.3059, 0.2784, 0.5765, 0.3647, 0.9647,\n",
       "           0.5333, 0.4235, 0.3412, 0.0275, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3176, 0.4980, 0.4000, 0.3333, 0.5765, 0.3804, 0.9922,\n",
       "           0.5412, 0.3922, 0.3255, 0.0588, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2745, 0.5373, 0.3922, 0.4353, 0.5922, 0.1922, 0.9882,\n",
       "           0.5765, 0.4157, 0.3373, 0.0392, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2392, 0.5294, 0.3451, 0.3922, 0.5451, 0.0000, 0.9882,\n",
       "           0.6118, 0.3804, 0.3098, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2275, 0.5098, 0.3412, 0.3882, 0.4980, 0.0000, 0.9569,\n",
       "           0.6353, 0.3373, 0.2941, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1490, 0.5059, 0.3490, 0.4118, 0.4667, 0.0000, 0.9059,\n",
       "           0.6392, 0.3059, 0.1922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0510, 0.5294, 0.3529, 0.4510, 0.4863, 0.0000, 0.9216,\n",
       "           0.5961, 0.2510, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0118, 0.4902, 0.3059, 0.4588, 0.5569, 0.0000, 0.8784,\n",
       "           0.6235, 0.2235, 0.1255, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.4471, 0.3137, 0.4745, 0.4745, 0.0000, 0.7333,\n",
       "           0.6588, 0.2431, 0.1569, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.4627, 0.3843, 0.4902, 0.4235, 0.0000, 0.5686,\n",
       "           0.6784, 0.2980, 0.2078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.5373, 0.3176, 0.6706, 0.2667, 0.0000, 0.3725,\n",
       "           0.7804, 0.1922, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.7843, 0.6745, 0.9608, 0.0314, 0.0000, 0.2000,\n",
       "           0.8510, 0.6118, 0.4314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.7961, 0.8196, 0.8196, 0.0000, 0.0000, 0.1843,\n",
       "           0.9098, 0.8275, 0.6314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.8196, 0.8549, 0.8275, 0.0000, 0.0000, 0.2078,\n",
       "           0.9255, 0.8863, 0.6471, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.7961, 0.8745, 0.8118, 0.0000, 0.0000, 0.1882,\n",
       "           0.9137, 0.9059, 0.5529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.7608, 0.9059, 0.7804, 0.0000, 0.0000, 0.1647,\n",
       "           0.9176, 0.9137, 0.5333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.8549, 0.9176, 0.7882, 0.0000, 0.0000, 0.1176,\n",
       "           0.9098, 0.9216, 0.4980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.7647, 0.9412, 0.7725, 0.0000, 0.0000, 0.0510,\n",
       "           0.8980, 0.9373, 0.4118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.4784, 0.9686, 0.7647, 0.0000, 0.0000, 0.0000,\n",
       "           0.8431, 0.9294, 0.2902, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.4471, 1.0000, 0.7529, 0.0000, 0.0000, 0.0000,\n",
       "           0.8784, 0.9725, 0.1804, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0784, 0.6824, 0.3961, 0.0000, 0.0000, 0.0000,\n",
       "           0.5725, 0.5725, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[3][0], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a56c714b-e788-4ced-8866-4f76c239fa8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trouser'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[test_data[3][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94b56d2c-96b9-4773-bdb6-528f2c3e6770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trouser'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[model(test_data[3][0]).argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc67bf6d-0e6e-4899-ac65-5e7cd7729514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
